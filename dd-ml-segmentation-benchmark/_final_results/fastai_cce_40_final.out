Running inference...
Creating inference object for baseline_model from models
loading model baseline_model ...
running inference on image dataset-medium/images/1d4fbe33f3_F1BE1D4184INSPIRE-ortho.tif.
loading input image (5542, 2962, 3)
running inference on chip 0 of 15
running inference on chip 1 of 15
running inference on chip 2 of 15
running inference on chip 3 of 15
running inference on chip 4 of 15
running inference on chip 5 of 15
running inference on chip 6 of 15
running inference on chip 7 of 15
running inference on chip 8 of 15
running inference on chip 9 of 15
running inference on chip 10 of 15
running inference on chip 11 of 15
running inference on chip 12 of 15
running inference on chip 13 of 15
running inference on chip 14 of 15
running inference on image dataset-medium/images/1df70e7340_4413A67E91INSPIRE-ortho.tif.
loading input image (3057, 4777, 3)
running inference on chip 0 of 12
running inference on chip 1 of 12
running inference on chip 2 of 12
running inference on chip 3 of 12
running inference on chip 4 of 12
running inference on chip 5 of 12
running inference on chip 6 of 12
running inference on chip 7 of 12
running inference on chip 8 of 12
running inference on chip 9 of 12
running inference on chip 10 of 12
running inference on chip 11 of 12
running inference on image dataset-medium/images/7008b80b00_FF24A4975DINSPIRE-ortho.tif.
loading input image (4395, 5159, 3)
running inference on chip 0 of 20
running inference on chip 1 of 20
running inference on chip 2 of 20
running inference on chip 3 of 20
running inference on chip 4 of 20
running inference on chip 5 of 20
running inference on chip 6 of 20
running inference on chip 7 of 20
running inference on chip 8 of 20
running inference on chip 9 of 20
running inference on chip 10 of 20
running inference on chip 11 of 20
running inference on chip 12 of 20
running inference on chip 13 of 20
running inference on chip 14 of 20
running inference on chip 15 of 20
running inference on chip 16 of 20
running inference on chip 17 of 20
running inference on chip 18 of 20
running inference on chip 19 of 20
running inference on image dataset-medium/images/c644f91210_27E21B7F30OPENPIPELINE-ortho.tif.
loading input image (9364, 4682, 3)
running inference on chip 0 of 32
running inference on chip 1 of 32
running inference on chip 2 of 32
running inference on chip 3 of 32
running inference on chip 4 of 32
running inference on chip 5 of 32
running inference on chip 6 of 32
running inference on chip 7 of 32
running inference on chip 8 of 32
running inference on chip 9 of 32
running inference on chip 10 of 32
running inference on chip 11 of 32
running inference on chip 12 of 32
running inference on chip 13 of 32
running inference on chip 14 of 32
running inference on chip 15 of 32
running inference on chip 16 of 32
running inference on chip 17 of 32
running inference on chip 18 of 32
running inference on chip 19 of 32
running inference on chip 20 of 32
running inference on chip 21 of 32
running inference on chip 22 of 32
running inference on chip 23 of 32
running inference on chip 24 of 32
running inference on chip 25 of 32
running inference on chip 26 of 32
running inference on chip 27 of 32
running inference on chip 28 of 32
running inference on chip 29 of 32
running inference on chip 30 of 32
running inference on chip 31 of 32
running inference on image dataset-medium/images/b705d0cc9c_E5F5E0E316OPENPIPELINE-ortho.tif.
loading input image (7453, 5924, 3)
running inference on chip 0 of 35
running inference on chip 1 of 35
running inference on chip 2 of 35
running inference on chip 3 of 35
running inference on chip 4 of 35
running inference on chip 5 of 35
running inference on chip 6 of 35
running inference on chip 7 of 35
running inference on chip 8 of 35
running inference on chip 9 of 35
running inference on chip 10 of 35
running inference on chip 11 of 35
running inference on chip 12 of 35
running inference on chip 13 of 35
running inference on chip 14 of 35
running inference on chip 15 of 35
running inference on chip 16 of 35
running inference on chip 17 of 35
running inference on chip 18 of 35
running inference on chip 19 of 35
running inference on chip 20 of 35
running inference on chip 21 of 35
running inference on chip 22 of 35
running inference on chip 23 of 35
running inference on chip 24 of 35
running inference on chip 25 of 35
running inference on chip 26 of 35
running inference on chip 27 of 35
running inference on chip 28 of 35
running inference on chip 29 of 35
running inference on chip 30 of 35
running inference on chip 31 of 35
running inference on chip 32 of 35
running inference on chip 33 of 35
running inference on chip 34 of 35
running inference on image dataset-medium/images/a1af86939f_F1BE1D4184OPENPIPELINE-ortho.tif.
loading input image (1911, 2102, 3)
running inference on chip 0 of 4
running inference on chip 1 of 4
running inference on chip 2 of 4
running inference on chip 3 of 4
running inference on image dataset-medium/images/520947aa07_8FCB044F58OPENPIPELINE-ortho.tif.
loading input image (4968, 4968, 3)
running inference on chip 0 of 25
running inference on chip 1 of 25
running inference on chip 2 of 25
running inference on chip 3 of 25
running inference on chip 4 of 25
running inference on chip 5 of 25
running inference on chip 6 of 25
running inference on chip 7 of 25
running inference on chip 8 of 25
running inference on chip 9 of 25
running inference on chip 10 of 25
running inference on chip 11 of 25
running inference on chip 12 of 25
running inference on chip 13 of 25
running inference on chip 14 of 25
running inference on chip 15 of 25
running inference on chip 16 of 25
running inference on chip 17 of 25
running inference on chip 18 of 25
running inference on chip 19 of 25
running inference on chip 20 of 25
running inference on chip 21 of 25
running inference on chip 22 of 25
running inference on chip 23 of 25
running inference on chip 24 of 25
running inference on image dataset-medium/images/2ef883f08d_F317F9C1DFOPENPIPELINE-ortho.tif.
loading input image (7644, 6115, 3)
running inference on chip 0 of 42
running inference on chip 1 of 42
running inference on chip 2 of 42
running inference on chip 3 of 42
running inference on chip 4 of 42
running inference on chip 5 of 42
running inference on chip 6 of 42
running inference on chip 7 of 42
running inference on chip 8 of 42
running inference on chip 9 of 42
running inference on chip 10 of 42
running inference on chip 11 of 42
running inference on chip 12 of 42
running inference on chip 13 of 42
running inference on chip 14 of 42
running inference on chip 15 of 42
running inference on chip 16 of 42
running inference on chip 17 of 42
running inference on chip 18 of 42
running inference on chip 19 of 42
running inference on chip 20 of 42
running inference on chip 21 of 42
running inference on chip 22 of 42
running inference on chip 23 of 42
running inference on chip 24 of 42
running inference on chip 25 of 42
running inference on chip 26 of 42
running inference on chip 27 of 42
running inference on chip 28 of 42
running inference on chip 29 of 42
running inference on chip 30 of 42
running inference on chip 31 of 42
running inference on chip 32 of 42
running inference on chip 33 of 42
running inference on chip 34 of 42
running inference on chip 35 of 42
running inference on chip 36 of 42
running inference on chip 37 of 42
running inference on chip 38 of 42
running inference on chip 39 of 42
running inference on chip 40 of 42
running inference on chip 41 of 42
running inference on image dataset-medium/images/f971256246_MIKEINSPIRE-ortho.tif.
loading input image (4682, 4586, 3)
running inference on chip 0 of 16
running inference on chip 1 of 16
running inference on chip 2 of 16
running inference on chip 3 of 16
running inference on chip 4 of 16
running inference on chip 5 of 16
running inference on chip 6 of 16
running inference on chip 7 of 16
running inference on chip 8 of 16
running inference on chip 9 of 16
running inference on chip 10 of 16
running inference on chip 11 of 16
running inference on chip 12 of 16
running inference on chip 13 of 16
running inference on chip 14 of 16
running inference on chip 15 of 16
running inference on image dataset-medium/images/2ef3a4994a_0CCD105428INSPIRE-ortho.tif.
loading input image (2675, 3249, 3)
running inference on chip 0 of 9
running inference on chip 1 of 9
running inference on chip 2 of 9
running inference on chip 3 of 9
running inference on chip 4 of 9
running inference on chip 5 of 9
running inference on chip 6 of 9
running inference on chip 7 of 9
running inference on chip 8 of 9
running inference on image dataset-medium/images/888432f840_80E7FD39EBINSPIRE-ortho.tif.
loading input image (7835, 12421, 3)
running inference on chip 0 of 77
running inference on chip 1 of 77
running inference on chip 2 of 77
running inference on chip 3 of 77
running inference on chip 4 of 77
running inference on chip 5 of 77
running inference on chip 6 of 77
running inference on chip 7 of 77
running inference on chip 8 of 77
running inference on chip 9 of 77
running inference on chip 10 of 77
running inference on chip 11 of 77
running inference on chip 12 of 77
running inference on chip 13 of 77
running inference on chip 14 of 77
running inference on chip 15 of 77
running inference on chip 16 of 77
running inference on chip 17 of 77
running inference on chip 18 of 77
running inference on chip 19 of 77
running inference on chip 20 of 77
running inference on chip 21 of 77
running inference on chip 22 of 77
running inference on chip 23 of 77
running inference on chip 24 of 77
running inference on chip 25 of 77
running inference on chip 26 of 77
running inference on chip 27 of 77
running inference on chip 28 of 77
running inference on chip 29 of 77
running inference on chip 30 of 77
running inference on chip 31 of 77
running inference on chip 32 of 77
running inference on chip 33 of 77
running inference on chip 34 of 77
running inference on chip 35 of 77
running inference on chip 36 of 77
running inference on chip 37 of 77
running inference on chip 38 of 77
running inference on chip 39 of 77
running inference on chip 40 of 77
running inference on chip 41 of 77
running inference on chip 42 of 77
running inference on chip 43 of 77
running inference on chip 44 of 77
running inference on chip 45 of 77
running inference on chip 46 of 77
running inference on chip 47 of 77
running inference on chip 48 of 77
running inference on chip 49 of 77
running inference on chip 50 of 77
running inference on chip 51 of 77
running inference on chip 52 of 77
running inference on chip 53 of 77
running inference on chip 54 of 77
running inference on chip 55 of 77
running inference on chip 56 of 77
running inference on chip 57 of 77
running inference on chip 58 of 77
running inference on chip 59 of 77
running inference on chip 60 of 77
running inference on chip 61 of 77
running inference on chip 62 of 77
running inference on chip 63 of 77
running inference on chip 64 of 77
running inference on chip 65 of 77
running inference on chip 66 of 77
running inference on chip 67 of 77
running inference on chip 68 of 77
running inference on chip 69 of 77
running inference on chip 70 of 77
running inference on chip 71 of 77
running inference on chip 72 of 77
running inference on chip 73 of 77
running inference on chip 74 of 77
running inference on chip 75 of 77
running inference on chip 76 of 77
running inference on image dataset-medium/images/130a76ebe1_68B40B480AOPENPIPELINE-ortho.tif.
loading input image (8217, 5733, 3)
running inference on chip 0 of 35
running inference on chip 1 of 35
running inference on chip 2 of 35
running inference on chip 3 of 35
running inference on chip 4 of 35
running inference on chip 5 of 35
running inference on chip 6 of 35
running inference on chip 7 of 35
running inference on chip 8 of 35
running inference on chip 9 of 35
running inference on chip 10 of 35
running inference on chip 11 of 35
running inference on chip 12 of 35
running inference on chip 13 of 35
running inference on chip 14 of 35
running inference on chip 15 of 35
running inference on chip 16 of 35
running inference on chip 17 of 35
running inference on chip 18 of 35
running inference on chip 19 of 35
running inference on chip 20 of 35
running inference on chip 21 of 35
running inference on chip 22 of 35
running inference on chip 23 of 35
running inference on chip 24 of 35
running inference on chip 25 of 35
running inference on chip 26 of 35
running inference on chip 27 of 35
running inference on chip 28 of 35
running inference on chip 29 of 35
running inference on chip 30 of 35
running inference on chip 31 of 35
running inference on chip 32 of 35
running inference on chip 33 of 35
running inference on chip 34 of 35
running inference on image dataset-medium/images/11cdce7802_B6A62F8BE0INSPIRE-ortho.tif.
loading input image (3822, 8408, 3)
running inference on chip 0 of 32
running inference on chip 1 of 32
running inference on chip 2 of 32
running inference on chip 3 of 32
running inference on chip 4 of 32
running inference on chip 5 of 32
running inference on chip 6 of 32
running inference on chip 7 of 32
running inference on chip 8 of 32
running inference on chip 9 of 32
running inference on chip 10 of 32
running inference on chip 11 of 32
running inference on chip 12 of 32
running inference on chip 13 of 32
running inference on chip 14 of 32
running inference on chip 15 of 32
running inference on chip 16 of 32
running inference on chip 17 of 32
running inference on chip 18 of 32
running inference on chip 19 of 32
running inference on chip 20 of 32
running inference on chip 21 of 32
running inference on chip 22 of 32
running inference on chip 23 of 32
running inference on chip 24 of 32
running inference on chip 25 of 32
running inference on chip 26 of 32
running inference on chip 27 of 32
running inference on chip 28 of 32
running inference on chip 29 of 32
running inference on chip 30 of 32
running inference on chip 31 of 32
running inference on image dataset-medium/images/3502e187b2_23071E4605OPENPIPELINE-ortho.tif.
loading input image (8217, 5064, 3)
running inference on chip 0 of 35
running inference on chip 1 of 35
running inference on chip 2 of 35
running inference on chip 3 of 35
running inference on chip 4 of 35
running inference on chip 5 of 35
running inference on chip 6 of 35
running inference on chip 7 of 35
running inference on chip 8 of 35
running inference on chip 9 of 35
running inference on chip 10 of 35
running inference on chip 11 of 35
running inference on chip 12 of 35
running inference on chip 13 of 35
running inference on chip 14 of 35
running inference on chip 15 of 35
running inference on chip 16 of 35
running inference on chip 17 of 35
running inference on chip 18 of 35
running inference on chip 19 of 35
running inference on chip 20 of 35
running inference on chip 21 of 35
running inference on chip 22 of 35
running inference on chip 23 of 35
running inference on chip 24 of 35
running inference on chip 25 of 35
running inference on chip 26 of 35
running inference on chip 27 of 35
running inference on chip 28 of 35
running inference on chip 29 of 35
running inference on chip 30 of 35
running inference on chip 31 of 35
running inference on chip 32 of 35
running inference on chip 33 of 35
running inference on chip 34 of 35
running inference on image dataset-medium/images/f9f43e5144_1DB9E6F68BINSPIRE-ortho.tif.
loading input image (6688, 12039, 3)
running inference on chip 0 of 66
running inference on chip 1 of 66
running inference on chip 2 of 66
running inference on chip 3 of 66
running inference on chip 4 of 66
running inference on chip 5 of 66
running inference on chip 6 of 66
running inference on chip 7 of 66
running inference on chip 8 of 66
running inference on chip 9 of 66
running inference on chip 10 of 66
running inference on chip 11 of 66
running inference on chip 12 of 66
running inference on chip 13 of 66
running inference on chip 14 of 66
running inference on chip 15 of 66
running inference on chip 16 of 66
running inference on chip 17 of 66
running inference on chip 18 of 66
running inference on chip 19 of 66
running inference on chip 20 of 66
running inference on chip 21 of 66
running inference on chip 22 of 66
running inference on chip 23 of 66
running inference on chip 24 of 66
running inference on chip 25 of 66
running inference on chip 26 of 66
running inference on chip 27 of 66
running inference on chip 28 of 66
running inference on chip 29 of 66
running inference on chip 30 of 66
running inference on chip 31 of 66
running inference on chip 32 of 66
running inference on chip 33 of 66
running inference on chip 34 of 66
running inference on chip 35 of 66
running inference on chip 36 of 66
running inference on chip 37 of 66
running inference on chip 38 of 66
running inference on chip 39 of 66
running inference on chip 40 of 66
running inference on chip 41 of 66
running inference on chip 42 of 66
running inference on chip 43 of 66
running inference on chip 44 of 66
running inference on chip 45 of 66
running inference on chip 46 of 66
running inference on chip 47 of 66
running inference on chip 48 of 66
running inference on chip 49 of 66
running inference on chip 50 of 66
running inference on chip 51 of 66
running inference on chip 52 of 66
running inference on chip 53 of 66
running inference on chip 54 of 66
running inference on chip 55 of 66
running inference on chip 56 of 66
running inference on chip 57 of 66
running inference on chip 58 of 66
running inference on chip 59 of 66
running inference on chip 60 of 66
running inference on chip 61 of 66
running inference on chip 62 of 66
running inference on chip 63 of 66
running inference on chip 64 of 66
running inference on chip 65 of 66
running inference on image dataset-medium/images/1553627230_APIGENERATED-ortho.tif.
loading input image (12230, 4013, 3)
running inference on chip 0 of 44
running inference on chip 1 of 44
running inference on chip 2 of 44
running inference on chip 3 of 44
running inference on chip 4 of 44
running inference on chip 5 of 44
running inference on chip 6 of 44
running inference on chip 7 of 44
running inference on chip 8 of 44
running inference on chip 9 of 44
running inference on chip 10 of 44
running inference on chip 11 of 44
running inference on chip 12 of 44
running inference on chip 13 of 44
running inference on chip 14 of 44
running inference on chip 15 of 44
running inference on chip 16 of 44
running inference on chip 17 of 44
running inference on chip 18 of 44
running inference on chip 19 of 44
running inference on chip 20 of 44
running inference on chip 21 of 44
running inference on chip 22 of 44
running inference on chip 23 of 44
running inference on chip 24 of 44
running inference on chip 25 of 44
running inference on chip 26 of 44
running inference on chip 27 of 44
running inference on chip 28 of 44
running inference on chip 29 of 44
running inference on chip 30 of 44
running inference on chip 31 of 44
running inference on chip 32 of 44
running inference on chip 33 of 44
running inference on chip 34 of 44
running inference on chip 35 of 44
running inference on chip 36 of 44
running inference on chip 37 of 44
running inference on chip 38 of 44
running inference on chip 39 of 44
running inference on chip 40 of 44
running inference on chip 41 of 44
running inference on chip 42 of 44
running inference on chip 43 of 44
running inference on image dataset-medium/images/ebffe540d0_7BA042D858OPENPIPELINE-ortho.tif.
loading input image (2867, 2389, 3)
running inference on chip 0 of 6
running inference on chip 1 of 6
running inference on chip 2 of 6
running inference on chip 3 of 6
running inference on chip 4 of 6
running inference on chip 5 of 6
running inference on image dataset-medium/images/d9161f7e18_C05BA1BC72OPENPIPELINE-ortho.tif.
loading input image (3153, 3440, 3)
running inference on chip 0 of 9
running inference on chip 1 of 9
running inference on chip 2 of 9
running inference on chip 3 of 9
running inference on chip 4 of 9
running inference on chip 5 of 9
running inference on chip 6 of 9
running inference on chip 7 of 9
running inference on chip 8 of 9
running inference on image dataset-medium/images/34fbf7c2bd_E8AD935CEDINSPIRE-ortho.tif.
loading input image (1911, 19109, 3)
running inference on chip 0 of 32
running inference on chip 1 of 32
running inference on chip 2 of 32
running inference on chip 3 of 32
running inference on chip 4 of 32
running inference on chip 5 of 32
running inference on chip 6 of 32
running inference on chip 7 of 32
running inference on chip 8 of 32
running inference on chip 9 of 32
running inference on chip 10 of 32
running inference on chip 11 of 32
running inference on chip 12 of 32
running inference on chip 13 of 32
running inference on chip 14 of 32
running inference on chip 15 of 32
running inference on chip 16 of 32
running inference on chip 17 of 32
running inference on chip 18 of 32
running inference on chip 19 of 32
running inference on chip 20 of 32
running inference on chip 21 of 32
running inference on chip 22 of 32
running inference on chip 23 of 32
running inference on chip 24 of 32
running inference on chip 25 of 32
running inference on chip 26 of 32
running inference on chip 27 of 32
running inference on chip 28 of 32
running inference on chip 29 of 32
running inference on chip 30 of 32
running inference on chip 31 of 32
running inference on image dataset-medium/images/15efe45820_D95DF0B1F4INSPIRE-ortho.tif.
loading input image (12039, 13854, 3)
running inference on chip 0 of 132
running inference on chip 1 of 132
running inference on chip 2 of 132
running inference on chip 3 of 132
running inference on chip 4 of 132
running inference on chip 5 of 132
running inference on chip 6 of 132
running inference on chip 7 of 132
running inference on chip 8 of 132
running inference on chip 9 of 132
running inference on chip 10 of 132
running inference on chip 11 of 132
running inference on chip 12 of 132
running inference on chip 13 of 132
running inference on chip 14 of 132
running inference on chip 15 of 132
running inference on chip 16 of 132
running inference on chip 17 of 132
running inference on chip 18 of 132
running inference on chip 19 of 132
running inference on chip 20 of 132
running inference on chip 21 of 132
running inference on chip 22 of 132
running inference on chip 23 of 132
running inference on chip 24 of 132
running inference on chip 25 of 132
running inference on chip 26 of 132
running inference on chip 27 of 132
running inference on chip 28 of 132
running inference on chip 29 of 132
running inference on chip 30 of 132
running inference on chip 31 of 132
running inference on chip 32 of 132
running inference on chip 33 of 132
running inference on chip 34 of 132
running inference on chip 35 of 132
running inference on chip 36 of 132
running inference on chip 37 of 132
running inference on chip 38 of 132
running inference on chip 39 of 132
running inference on chip 40 of 132
running inference on chip 41 of 132
running inference on chip 42 of 132
running inference on chip 43 of 132
running inference on chip 44 of 132
running inference on chip 45 of 132
running inference on chip 46 of 132
running inference on chip 47 of 132
running inference on chip 48 of 132
running inference on chip 49 of 132
running inference on chip 50 of 132
running inference on chip 51 of 132
running inference on chip 52 of 132
running inference on chip 53 of 132
running inference on chip 54 of 132
running inference on chip 55 of 132
running inference on chip 56 of 132
running inference on chip 57 of 132
running inference on chip 58 of 132
running inference on chip 59 of 132
running inference on chip 60 of 132
running inference on chip 61 of 132
running inference on chip 62 of 132
running inference on chip 63 of 132
running inference on chip 64 of 132
running inference on chip 65 of 132
running inference on chip 66 of 132
running inference on chip 67 of 132
running inference on chip 68 of 132
running inference on chip 69 of 132
running inference on chip 70 of 132
running inference on chip 71 of 132
running inference on chip 72 of 132
running inference on chip 73 of 132
running inference on chip 74 of 132
running inference on chip 75 of 132
running inference on chip 76 of 132
running inference on chip 77 of 132
running inference on chip 78 of 132
running inference on chip 79 of 132
running inference on chip 80 of 132
running inference on chip 81 of 132
running inference on chip 82 of 132
running inference on chip 83 of 132
running inference on chip 84 of 132
running inference on chip 85 of 132
running inference on chip 86 of 132
running inference on chip 87 of 132
running inference on chip 88 of 132
running inference on chip 89 of 132
running inference on chip 90 of 132
running inference on chip 91 of 132
running inference on chip 92 of 132
running inference on chip 93 of 132
running inference on chip 94 of 132
running inference on chip 95 of 132
running inference on chip 96 of 132
running inference on chip 97 of 132
running inference on chip 98 of 132
running inference on chip 99 of 132
running inference on chip 100 of 132
running inference on chip 101 of 132
running inference on chip 102 of 132
running inference on chip 103 of 132
running inference on chip 104 of 132
running inference on chip 105 of 132
running inference on chip 106 of 132
running inference on chip 107 of 132
running inference on chip 108 of 132
running inference on chip 109 of 132
running inference on chip 110 of 132
running inference on chip 111 of 132
running inference on chip 112 of 132
running inference on chip 113 of 132
running inference on chip 114 of 132
running inference on chip 115 of 132
running inference on chip 116 of 132
running inference on chip 117 of 132
running inference on chip 118 of 132
running inference on chip 119 of 132
running inference on chip 120 of 132
running inference on chip 121 of 132
running inference on chip 122 of 132
running inference on chip 123 of 132
running inference on chip 124 of 132
running inference on chip 125 of 132
running inference on chip 126 of 132
running inference on chip 127 of 132
running inference on chip 128 of 132
running inference on chip 129 of 132
running inference on chip 130 of 132
running inference on chip 131 of 132
running inference on image dataset-medium/images/2552eb56dd_2AABB46C86OPENPIPELINE-ortho.tif.
loading input image (3631, 1051, 3)
running inference on chip 0 of 4
running inference on chip 1 of 4
running inference on chip 2 of 4
running inference on chip 3 of 4
running inference on image dataset-medium/images/1553541487_APIGENERATED-ortho.tif.
loading input image (9364, 5351, 3)
running inference on chip 0 of 40
running inference on chip 1 of 40
running inference on chip 2 of 40
running inference on chip 3 of 40
running inference on chip 4 of 40
running inference on chip 5 of 40
running inference on chip 6 of 40
running inference on chip 7 of 40
running inference on chip 8 of 40
running inference on chip 9 of 40
running inference on chip 10 of 40
running inference on chip 11 of 40
running inference on chip 12 of 40
running inference on chip 13 of 40
running inference on chip 14 of 40
running inference on chip 15 of 40
running inference on chip 16 of 40
running inference on chip 17 of 40
running inference on chip 18 of 40
running inference on chip 19 of 40
running inference on chip 20 of 40
running inference on chip 21 of 40
running inference on chip 22 of 40
running inference on chip 23 of 40
running inference on chip 24 of 40
running inference on chip 25 of 40
running inference on chip 26 of 40
running inference on chip 27 of 40
running inference on chip 28 of 40
running inference on chip 29 of 40
running inference on chip 30 of 40
running inference on chip 31 of 40
running inference on chip 32 of 40
running inference on chip 33 of 40
running inference on chip 34 of 40
running inference on chip 35 of 40
running inference on chip 36 of 40
running inference on chip 37 of 40
running inference on chip 38 of 40
running inference on chip 39 of 40
running inference on image dataset-medium/images/84410645db_8D20F02042OPENPIPELINE-ortho.tif.
loading input image (10415, 11752, 3)
running inference on chip 0 of 90
running inference on chip 1 of 90
running inference on chip 2 of 90
running inference on chip 3 of 90
running inference on chip 4 of 90
running inference on chip 5 of 90
running inference on chip 6 of 90
running inference on chip 7 of 90
running inference on chip 8 of 90
running inference on chip 9 of 90
running inference on chip 10 of 90
running inference on chip 11 of 90
running inference on chip 12 of 90
running inference on chip 13 of 90
running inference on chip 14 of 90
running inference on chip 15 of 90
running inference on chip 16 of 90
running inference on chip 17 of 90
running inference on chip 18 of 90
running inference on chip 19 of 90
running inference on chip 20 of 90
running inference on chip 21 of 90
running inference on chip 22 of 90
running inference on chip 23 of 90
running inference on chip 24 of 90
running inference on chip 25 of 90
running inference on chip 26 of 90
running inference on chip 27 of 90
running inference on chip 28 of 90
running inference on chip 29 of 90
running inference on chip 30 of 90
running inference on chip 31 of 90
running inference on chip 32 of 90
running inference on chip 33 of 90
running inference on chip 34 of 90
running inference on chip 35 of 90
running inference on chip 36 of 90
running inference on chip 37 of 90
running inference on chip 38 of 90
running inference on chip 39 of 90
running inference on chip 40 of 90
running inference on chip 41 of 90
running inference on chip 42 of 90
running inference on chip 43 of 90
running inference on chip 44 of 90
running inference on chip 45 of 90
running inference on chip 46 of 90
running inference on chip 47 of 90
running inference on chip 48 of 90
running inference on chip 49 of 90
running inference on chip 50 of 90
running inference on chip 51 of 90
running inference on chip 52 of 90
running inference on chip 53 of 90
running inference on chip 54 of 90
running inference on chip 55 of 90
running inference on chip 56 of 90
running inference on chip 57 of 90
running inference on chip 58 of 90
running inference on chip 59 of 90
running inference on chip 60 of 90
running inference on chip 61 of 90
running inference on chip 62 of 90
running inference on chip 63 of 90
running inference on chip 64 of 90
running inference on chip 65 of 90
running inference on chip 66 of 90
running inference on chip 67 of 90
running inference on chip 68 of 90
running inference on chip 69 of 90
running inference on chip 70 of 90
running inference on chip 71 of 90
running inference on chip 72 of 90
running inference on chip 73 of 90
running inference on chip 74 of 90
running inference on chip 75 of 90
running inference on chip 76 of 90
running inference on chip 77 of 90
running inference on chip 78 of 90
running inference on chip 79 of 90
running inference on chip 80 of 90
running inference on chip 81 of 90
running inference on chip 82 of 90
running inference on chip 83 of 90
running inference on chip 84 of 90
running inference on chip 85 of 90
running inference on chip 86 of 90
running inference on chip 87 of 90
running inference on chip 88 of 90
running inference on chip 89 of 90
running inference on image dataset-medium/images/f0747ed88d_E74C0DD8FDOPENPIPELINE-ortho.tif.
loading input image (1529, 1433, 3)
running inference on chip 0 of 4
running inference on chip 1 of 4
running inference on chip 2 of 4
running inference on chip 3 of 4
running inference on image dataset-medium/images/c6d131e346_536DE05ED2OPENPIPELINE-ortho.tif.
loading input image (3249, 3822, 3)
running inference on chip 0 of 12
running inference on chip 1 of 12
running inference on chip 2 of 12
running inference on chip 3 of 12
running inference on chip 4 of 12
running inference on chip 5 of 12
running inference on chip 6 of 12
running inference on chip 7 of 12
running inference on chip 8 of 12
running inference on chip 9 of 12
running inference on chip 10 of 12
running inference on chip 11 of 12
running inference on image dataset-medium/images/b61673f780_4413A67E91INSPIRE-ortho.tif.
loading input image (3822, 4204, 3)
running inference on chip 0 of 16
running inference on chip 1 of 16
running inference on chip 2 of 16
running inference on chip 3 of 16
running inference on chip 4 of 16
running inference on chip 5 of 16
running inference on chip 6 of 16
running inference on chip 7 of 16
running inference on chip 8 of 16
running inference on chip 9 of 16
running inference on chip 10 of 16
running inference on chip 11 of 16
running inference on chip 12 of 16
running inference on chip 13 of 16
running inference on chip 14 of 16
running inference on chip 15 of 16
running inference on image dataset-medium/images/7c719dfcc0_310490364FINSPIRE-ortho.tif.
loading input image (4395, 3440, 3)
running inference on chip 0 of 12
running inference on chip 1 of 12
running inference on chip 2 of 12
running inference on chip 3 of 12
running inference on chip 4 of 12
running inference on chip 5 of 12
running inference on chip 6 of 12
running inference on chip 7 of 12
running inference on chip 8 of 12
running inference on chip 9 of 12
running inference on chip 10 of 12
running inference on chip 11 of 12
running inference on image dataset-medium/images/f56b6b2232_2A62B67B52OPENPIPELINE-ortho.tif.
loading input image (2293, 2484, 3)
running inference on chip 0 of 6
running inference on chip 1 of 6
running inference on chip 2 of 6
running inference on chip 3 of 6
running inference on chip 4 of 6
running inference on chip 5 of 6
running inference on image dataset-medium/images/c37dbfae2f_84B52814D2OPENPIPELINE-ortho.tif.
loading input image (3631, 12039, 3)
running inference on chip 0 of 44
running inference on chip 1 of 44
running inference on chip 2 of 44
running inference on chip 3 of 44
running inference on chip 4 of 44
running inference on chip 5 of 44
running inference on chip 6 of 44
running inference on chip 7 of 44
running inference on chip 8 of 44
running inference on chip 9 of 44
running inference on chip 10 of 44
running inference on chip 11 of 44
running inference on chip 12 of 44
running inference on chip 13 of 44
running inference on chip 14 of 44
running inference on chip 15 of 44
running inference on chip 16 of 44
running inference on chip 17 of 44
running inference on chip 18 of 44
running inference on chip 19 of 44
running inference on chip 20 of 44
running inference on chip 21 of 44
running inference on chip 22 of 44
running inference on chip 23 of 44
running inference on chip 24 of 44
running inference on chip 25 of 44
running inference on chip 26 of 44
running inference on chip 27 of 44
running inference on chip 28 of 44
running inference on chip 29 of 44
running inference on chip 30 of 44
running inference on chip 31 of 44
running inference on chip 32 of 44
running inference on chip 33 of 44
running inference on chip 34 of 44
running inference on chip 35 of 44
running inference on chip 36 of 44
running inference on chip 37 of 44
running inference on chip 38 of 44
running inference on chip 39 of 44
running inference on chip 40 of 44
running inference on chip 41 of 44
running inference on chip 42 of 44
running inference on chip 43 of 44
running inference on image dataset-medium/images/5fa39d6378_DB9FF730D9OPENPIPELINE-ortho.tif.
loading input image (9077, 9077, 3)
running inference on chip 0 of 64
running inference on chip 1 of 64
running inference on chip 2 of 64
running inference on chip 3 of 64
running inference on chip 4 of 64
running inference on chip 5 of 64
running inference on chip 6 of 64
running inference on chip 7 of 64
running inference on chip 8 of 64
running inference on chip 9 of 64
running inference on chip 10 of 64
running inference on chip 11 of 64
running inference on chip 12 of 64
running inference on chip 13 of 64
running inference on chip 14 of 64
running inference on chip 15 of 64
running inference on chip 16 of 64
running inference on chip 17 of 64
running inference on chip 18 of 64
running inference on chip 19 of 64
running inference on chip 20 of 64
running inference on chip 21 of 64
running inference on chip 22 of 64
running inference on chip 23 of 64
running inference on chip 24 of 64
running inference on chip 25 of 64
running inference on chip 26 of 64
running inference on chip 27 of 64
running inference on chip 28 of 64
running inference on chip 29 of 64
running inference on chip 30 of 64
running inference on chip 31 of 64
running inference on chip 32 of 64
running inference on chip 33 of 64
running inference on chip 34 of 64
running inference on chip 35 of 64
running inference on chip 36 of 64
running inference on chip 37 of 64
running inference on chip 38 of 64
running inference on chip 39 of 64
running inference on chip 40 of 64
running inference on chip 41 of 64
running inference on chip 42 of 64
running inference on chip 43 of 64
running inference on chip 44 of 64
running inference on chip 45 of 64
running inference on chip 46 of 64
running inference on chip 47 of 64
running inference on chip 48 of 64
running inference on chip 49 of 64
running inference on chip 50 of 64
running inference on chip 51 of 64
running inference on chip 52 of 64
running inference on chip 53 of 64
running inference on chip 54 of 64
running inference on chip 55 of 64
running inference on chip 56 of 64
running inference on chip 57 of 64
running inference on chip 58 of 64
running inference on chip 59 of 64
running inference on chip 60 of 64
running inference on chip 61 of 64
running inference on chip 62 of 64
running inference on chip 63 of 64
running inference on image dataset-medium/images/f4dd768188_NOLANOPENPIPELINE-ortho.tif.
loading input image (4873, 4873, 3)
running inference on chip 0 of 25
running inference on chip 1 of 25
running inference on chip 2 of 25
running inference on chip 3 of 25
running inference on chip 4 of 25
running inference on chip 5 of 25
running inference on chip 6 of 25
running inference on chip 7 of 25
running inference on chip 8 of 25
running inference on chip 9 of 25
running inference on chip 10 of 25
running inference on chip 11 of 25
running inference on chip 12 of 25
running inference on chip 13 of 25
running inference on chip 14 of 25
running inference on chip 15 of 25
running inference on chip 16 of 25
running inference on chip 17 of 25
running inference on chip 18 of 25
running inference on chip 19 of 25
running inference on chip 20 of 25
running inference on chip 21 of 25
running inference on chip 22 of 25
running inference on chip 23 of 25
running inference on chip 24 of 25
running inference on image dataset-medium/images/b771104de5_7E02A41EBEOPENPIPELINE-ortho.tif.
loading input image (3344, 2293, 3)
running inference on chip 0 of 6
running inference on chip 1 of 6
running inference on chip 2 of 6
running inference on chip 3 of 6
running inference on chip 4 of 6
running inference on chip 5 of 6
running inference on image dataset-medium/images/1553541585_APIGENERATED-ortho.tif.
loading input image (8217, 8026, 3)
running inference on chip 0 of 49
running inference on chip 1 of 49
running inference on chip 2 of 49
running inference on chip 3 of 49
running inference on chip 4 of 49
running inference on chip 5 of 49
running inference on chip 6 of 49
running inference on chip 7 of 49
running inference on chip 8 of 49
running inference on chip 9 of 49
running inference on chip 10 of 49
running inference on chip 11 of 49
running inference on chip 12 of 49
running inference on chip 13 of 49
running inference on chip 14 of 49
running inference on chip 15 of 49
running inference on chip 16 of 49
running inference on chip 17 of 49
running inference on chip 18 of 49
running inference on chip 19 of 49
running inference on chip 20 of 49
running inference on chip 21 of 49
running inference on chip 22 of 49
running inference on chip 23 of 49
running inference on chip 24 of 49
running inference on chip 25 of 49
running inference on chip 26 of 49
running inference on chip 27 of 49
running inference on chip 28 of 49
running inference on chip 29 of 49
running inference on chip 30 of 49
running inference on chip 31 of 49
running inference on chip 32 of 49
running inference on chip 33 of 49
running inference on chip 34 of 49
running inference on chip 35 of 49
running inference on chip 36 of 49
running inference on chip 37 of 49
running inference on chip 38 of 49
running inference on chip 39 of 49
running inference on chip 40 of 49
running inference on chip 41 of 49
running inference on chip 42 of 49
running inference on chip 43 of 49
running inference on chip 44 of 49
running inference on chip 45 of 49
running inference on chip 46 of 49
running inference on chip 47 of 49
running inference on chip 48 of 49
running inference on image dataset-medium/images/1d056881e8_29FEA32BC7INSPIRE-ortho.tif.
loading input image (10701, 9937, 3)
running inference on chip 0 of 81
running inference on chip 1 of 81
running inference on chip 2 of 81
running inference on chip 3 of 81
running inference on chip 4 of 81
running inference on chip 5 of 81
running inference on chip 6 of 81
running inference on chip 7 of 81
running inference on chip 8 of 81
running inference on chip 9 of 81
running inference on chip 10 of 81
running inference on chip 11 of 81
running inference on chip 12 of 81
running inference on chip 13 of 81
running inference on chip 14 of 81
running inference on chip 15 of 81
running inference on chip 16 of 81
running inference on chip 17 of 81
running inference on chip 18 of 81
running inference on chip 19 of 81
running inference on chip 20 of 81
running inference on chip 21 of 81
running inference on chip 22 of 81
running inference on chip 23 of 81
running inference on chip 24 of 81
running inference on chip 25 of 81
running inference on chip 26 of 81
running inference on chip 27 of 81
running inference on chip 28 of 81
running inference on chip 29 of 81
running inference on chip 30 of 81
running inference on chip 31 of 81
running inference on chip 32 of 81
running inference on chip 33 of 81
running inference on chip 34 of 81
running inference on chip 35 of 81
running inference on chip 36 of 81
running inference on chip 37 of 81
running inference on chip 38 of 81
running inference on chip 39 of 81
running inference on chip 40 of 81
running inference on chip 41 of 81
running inference on chip 42 of 81
running inference on chip 43 of 81
running inference on chip 44 of 81
running inference on chip 45 of 81
running inference on chip 46 of 81
running inference on chip 47 of 81
running inference on chip 48 of 81
running inference on chip 49 of 81
running inference on chip 50 of 81
running inference on chip 51 of 81
running inference on chip 52 of 81
running inference on chip 53 of 81
running inference on chip 54 of 81
running inference on chip 55 of 81
running inference on chip 56 of 81
running inference on chip 57 of 81
running inference on chip 58 of 81
running inference on chip 59 of 81
running inference on chip 60 of 81
running inference on chip 61 of 81
running inference on chip 62 of 81
running inference on chip 63 of 81
running inference on chip 64 of 81
running inference on chip 65 of 81
running inference on chip 66 of 81
running inference on chip 67 of 81
running inference on chip 68 of 81
running inference on chip 69 of 81
running inference on chip 70 of 81
running inference on chip 71 of 81
running inference on chip 72 of 81
running inference on chip 73 of 81
running inference on chip 74 of 81
running inference on chip 75 of 81
running inference on chip 76 of 81
running inference on chip 77 of 81
running inference on chip 78 of 81
running inference on chip 79 of 81
running inference on chip 80 of 81
running inference on image dataset-medium/images/fc5837dcf8_7CD52BE09EINSPIRE-ortho.tif.
loading input image (4777, 4395, 3)
running inference on chip 0 of 16
running inference on chip 1 of 16
running inference on chip 2 of 16
running inference on chip 3 of 16
running inference on chip 4 of 16
running inference on chip 5 of 16
running inference on chip 6 of 16
running inference on chip 7 of 16
running inference on chip 8 of 16
running inference on chip 9 of 16
running inference on chip 10 of 16
running inference on chip 11 of 16
running inference on chip 12 of 16
running inference on chip 13 of 16
running inference on chip 14 of 16
running inference on chip 15 of 16
running inference on image dataset-medium/images/ec09336a6f_06BA0AF311OPENPIPELINE-ortho.tif.
loading input image (4968, 7262, 3)
running inference on chip 0 of 35
running inference on chip 1 of 35
running inference on chip 2 of 35
running inference on chip 3 of 35
running inference on chip 4 of 35
running inference on chip 5 of 35
running inference on chip 6 of 35
running inference on chip 7 of 35
running inference on chip 8 of 35
running inference on chip 9 of 35
running inference on chip 10 of 35
running inference on chip 11 of 35
running inference on chip 12 of 35
running inference on chip 13 of 35
running inference on chip 14 of 35
running inference on chip 15 of 35
running inference on chip 16 of 35
running inference on chip 17 of 35
running inference on chip 18 of 35
running inference on chip 19 of 35
running inference on chip 20 of 35
running inference on chip 21 of 35
running inference on chip 22 of 35
running inference on chip 23 of 35
running inference on chip 24 of 35
running inference on chip 25 of 35
running inference on chip 26 of 35
running inference on chip 27 of 35
running inference on chip 28 of 35
running inference on chip 29 of 35
running inference on chip 30 of 35
running inference on chip 31 of 35
running inference on chip 32 of 35
running inference on chip 33 of 35
running inference on chip 34 of 35
running inference on image dataset-medium/images/c8a7031e5f_32156F5DC2INSPIRE-ortho.tif.
loading input image (5159, 5733, 3)
running inference on chip 0 of 25
running inference on chip 1 of 25
running inference on chip 2 of 25
running inference on chip 3 of 25
running inference on chip 4 of 25
running inference on chip 5 of 25
running inference on chip 6 of 25
running inference on chip 7 of 25
running inference on chip 8 of 25
running inference on chip 9 of 25
running inference on chip 10 of 25
running inference on chip 11 of 25
running inference on chip 12 of 25
running inference on chip 13 of 25
running inference on chip 14 of 25
running inference on chip 15 of 25
running inference on chip 16 of 25
running inference on chip 17 of 25
running inference on chip 18 of 25
running inference on chip 19 of 25
running inference on chip 20 of 25
running inference on chip 21 of 25
running inference on chip 22 of 25
running inference on chip 23 of 25
running inference on chip 24 of 25
running inference on image dataset-medium/images/cc4b443c7d_A9CBEF2C97INSPIRE-ortho.tif.
loading input image (3440, 2866, 3)
running inference on chip 0 of 9
running inference on chip 1 of 9
running inference on chip 2 of 9
running inference on chip 3 of 9
running inference on chip 4 of 9
running inference on chip 5 of 9
running inference on chip 6 of 9
running inference on chip 7 of 9
running inference on chip 8 of 9
running inference on image dataset-medium/images/57426ebe1e_84B52814D2OPENPIPELINE-ortho.tif.
loading input image (3822, 3822, 3)
running inference on chip 0 of 16
running inference on chip 1 of 16
running inference on chip 2 of 16
running inference on chip 3 of 16
running inference on chip 4 of 16
running inference on chip 5 of 16
running inference on chip 6 of 16
running inference on chip 7 of 16
running inference on chip 8 of 16
running inference on chip 9 of 16
running inference on chip 10 of 16
running inference on chip 11 of 16
running inference on chip 12 of 16
running inference on chip 13 of 16
running inference on chip 14 of 16
running inference on chip 15 of 16
running inference on image dataset-medium/images/1476907971_CHADGRISMOPENPIPELINE-ortho.tif.
loading input image (5351, 8695, 3)
running inference on chip 0 of 40
running inference on chip 1 of 40
running inference on chip 2 of 40
running inference on chip 3 of 40
running inference on chip 4 of 40
running inference on chip 5 of 40
running inference on chip 6 of 40
running inference on chip 7 of 40
running inference on chip 8 of 40
running inference on chip 9 of 40
running inference on chip 10 of 40
running inference on chip 11 of 40
running inference on chip 12 of 40
running inference on chip 13 of 40
running inference on chip 14 of 40
running inference on chip 15 of 40
running inference on chip 16 of 40
running inference on chip 17 of 40
running inference on chip 18 of 40
running inference on chip 19 of 40
running inference on chip 20 of 40
running inference on chip 21 of 40
running inference on chip 22 of 40
running inference on chip 23 of 40
running inference on chip 24 of 40
running inference on chip 25 of 40
running inference on chip 26 of 40
running inference on chip 27 of 40
running inference on chip 28 of 40
running inference on chip 29 of 40
running inference on chip 30 of 40
running inference on chip 31 of 40
running inference on chip 32 of 40
running inference on chip 33 of 40
running inference on chip 34 of 40
running inference on chip 35 of 40
running inference on chip 36 of 40
running inference on chip 37 of 40
running inference on chip 38 of 40
running inference on chip 39 of 40
running inference on image dataset-medium/images/9170479165_625EDFBAB6OPENPIPELINE-ortho.tif.
loading input image (3631, 3440, 3)
running inference on chip 0 of 12
running inference on chip 1 of 12
running inference on chip 2 of 12
running inference on chip 3 of 12
running inference on chip 4 of 12
running inference on chip 5 of 12
running inference on chip 6 of 12
running inference on chip 7 of 12
running inference on chip 8 of 12
running inference on chip 9 of 12
running inference on chip 10 of 12
running inference on chip 11 of 12
running inference on image dataset-medium/images/551063e3c5_8FCB044F58INSPIRE-ortho.tif.
loading input image (1529, 1147, 3)
running inference on chip 0 of 2
running inference on chip 1 of 2
running inference on image dataset-medium/images/74d7796531_EB81FE6E2BOPENPIPELINE-ortho.tif.
loading input image (21211, 5924, 3)
running inference on chip 0 of 90
running inference on chip 1 of 90
running inference on chip 2 of 90
running inference on chip 3 of 90
running inference on chip 4 of 90
running inference on chip 5 of 90
running inference on chip 6 of 90
running inference on chip 7 of 90
running inference on chip 8 of 90
running inference on chip 9 of 90
running inference on chip 10 of 90
running inference on chip 11 of 90
running inference on chip 12 of 90
running inference on chip 13 of 90
running inference on chip 14 of 90
running inference on chip 15 of 90
running inference on chip 16 of 90
running inference on chip 17 of 90
running inference on chip 18 of 90
running inference on chip 19 of 90
running inference on chip 20 of 90
running inference on chip 21 of 90
running inference on chip 22 of 90
running inference on chip 23 of 90
running inference on chip 24 of 90
running inference on chip 25 of 90
running inference on chip 26 of 90
running inference on chip 27 of 90
running inference on chip 28 of 90
running inference on chip 29 of 90
running inference on chip 30 of 90
running inference on chip 31 of 90
running inference on chip 32 of 90
running inference on chip 33 of 90
running inference on chip 34 of 90
running inference on chip 35 of 90
running inference on chip 36 of 90
running inference on chip 37 of 90
running inference on chip 38 of 90
running inference on chip 39 of 90
running inference on chip 40 of 90
running inference on chip 41 of 90
running inference on chip 42 of 90
running inference on chip 43 of 90
running inference on chip 44 of 90
running inference on chip 45 of 90
running inference on chip 46 of 90
running inference on chip 47 of 90
running inference on chip 48 of 90
running inference on chip 49 of 90
running inference on chip 50 of 90
running inference on chip 51 of 90
running inference on chip 52 of 90
running inference on chip 53 of 90
running inference on chip 54 of 90
running inference on chip 55 of 90
running inference on chip 56 of 90
running inference on chip 57 of 90
running inference on chip 58 of 90
running inference on chip 59 of 90
running inference on chip 60 of 90
running inference on chip 61 of 90
running inference on chip 62 of 90
running inference on chip 63 of 90
running inference on chip 64 of 90
running inference on chip 65 of 90
running inference on chip 66 of 90
running inference on chip 67 of 90
running inference on chip 68 of 90
running inference on chip 69 of 90
running inference on chip 70 of 90
running inference on chip 71 of 90
running inference on chip 72 of 90
running inference on chip 73 of 90
running inference on chip 74 of 90
running inference on chip 75 of 90
running inference on chip 76 of 90
running inference on chip 77 of 90
running inference on chip 78 of 90
running inference on chip 79 of 90
running inference on chip 80 of 90
running inference on chip 81 of 90
running inference on chip 82 of 90
running inference on chip 83 of 90
running inference on chip 84 of 90
running inference on chip 85 of 90
running inference on chip 86 of 90
running inference on chip 87 of 90
running inference on chip 88 of 90
running inference on chip 89 of 90
running inference on image dataset-medium/images/12fa5e614f_53197F206FOPENPIPELINE-ortho.tif.
loading input image (5637, 5829, 3)
running inference on chip 0 of 25
running inference on chip 1 of 25
running inference on chip 2 of 25
running inference on chip 3 of 25
running inference on chip 4 of 25
running inference on chip 5 of 25
running inference on chip 6 of 25
running inference on chip 7 of 25
running inference on chip 8 of 25
running inference on chip 9 of 25
running inference on chip 10 of 25
running inference on chip 11 of 25
running inference on chip 12 of 25
running inference on chip 13 of 25
running inference on chip 14 of 25
running inference on chip 15 of 25
running inference on chip 16 of 25
running inference on chip 17 of 25
running inference on chip 18 of 25
running inference on chip 19 of 25
running inference on chip 20 of 25
running inference on chip 21 of 25
running inference on chip 22 of 25
running inference on chip 23 of 25
running inference on chip 24 of 25
running inference on image dataset-medium/images/c2e8370ca3_3340CAC7AEOPENPIPELINE-ortho.tif.
loading input image (2102, 3344, 3)
running inference on chip 0 of 6
running inference on chip 1 of 6
running inference on chip 2 of 6
running inference on chip 3 of 6
running inference on chip 4 of 6
running inference on chip 5 of 6
running inference on image dataset-medium/images/6f93b9026b_F1BFB8B17DOPENPIPELINE-ortho.tif.
loading input image (4682, 3918, 3)
running inference on chip 0 of 16
running inference on chip 1 of 16
running inference on chip 2 of 16
running inference on chip 3 of 16
running inference on chip 4 of 16
running inference on chip 5 of 16
running inference on chip 6 of 16
running inference on chip 7 of 16
running inference on chip 8 of 16
running inference on chip 9 of 16
running inference on chip 10 of 16
running inference on chip 11 of 16
running inference on chip 12 of 16
running inference on chip 13 of 16
running inference on chip 14 of 16
running inference on chip 15 of 16
running inference on image dataset-medium/images/8710b98ea0_06E6522D6DINSPIRE-ortho.tif.
loading input image (8217, 8408, 3)
running inference on chip 0 of 56
running inference on chip 1 of 56
running inference on chip 2 of 56
running inference on chip 3 of 56
running inference on chip 4 of 56
running inference on chip 5 of 56
running inference on chip 6 of 56
running inference on chip 7 of 56
running inference on chip 8 of 56
running inference on chip 9 of 56
running inference on chip 10 of 56
running inference on chip 11 of 56
running inference on chip 12 of 56
running inference on chip 13 of 56
running inference on chip 14 of 56
running inference on chip 15 of 56
running inference on chip 16 of 56
running inference on chip 17 of 56
running inference on chip 18 of 56
running inference on chip 19 of 56
running inference on chip 20 of 56
running inference on chip 21 of 56
running inference on chip 22 of 56
running inference on chip 23 of 56
running inference on chip 24 of 56
running inference on chip 25 of 56
running inference on chip 26 of 56
running inference on chip 27 of 56
running inference on chip 28 of 56
running inference on chip 29 of 56
running inference on chip 30 of 56
running inference on chip 31 of 56
running inference on chip 32 of 56
running inference on chip 33 of 56
running inference on chip 34 of 56
running inference on chip 35 of 56
running inference on chip 36 of 56
running inference on chip 37 of 56
running inference on chip 38 of 56
running inference on chip 39 of 56
running inference on chip 40 of 56
running inference on chip 41 of 56
running inference on chip 42 of 56
running inference on chip 43 of 56
running inference on chip 44 of 56
running inference on chip 45 of 56
running inference on chip 46 of 56
running inference on chip 47 of 56
running inference on chip 48 of 56
running inference on chip 49 of 56
running inference on chip 50 of 56
running inference on chip 51 of 56
running inference on chip 52 of 56
running inference on chip 53 of 56
running inference on chip 54 of 56
running inference on chip 55 of 56
running inference on image dataset-medium/images/25f1c24f30_EB81FE6E2BOPENPIPELINE-ortho.tif.
loading input image (11561, 6115, 3)
running inference on chip 0 of 60
running inference on chip 1 of 60
running inference on chip 2 of 60
running inference on chip 3 of 60
running inference on chip 4 of 60
running inference on chip 5 of 60
running inference on chip 6 of 60
running inference on chip 7 of 60
running inference on chip 8 of 60
running inference on chip 9 of 60
running inference on chip 10 of 60
running inference on chip 11 of 60
running inference on chip 12 of 60
running inference on chip 13 of 60
running inference on chip 14 of 60
running inference on chip 15 of 60
running inference on chip 16 of 60
running inference on chip 17 of 60
running inference on chip 18 of 60
running inference on chip 19 of 60
running inference on chip 20 of 60
running inference on chip 21 of 60
running inference on chip 22 of 60
running inference on chip 23 of 60
running inference on chip 24 of 60
running inference on chip 25 of 60
running inference on chip 26 of 60
running inference on chip 27 of 60
running inference on chip 28 of 60
running inference on chip 29 of 60
running inference on chip 30 of 60
running inference on chip 31 of 60
running inference on chip 32 of 60
running inference on chip 33 of 60
running inference on chip 34 of 60
running inference on chip 35 of 60
running inference on chip 36 of 60
running inference on chip 37 of 60
running inference on chip 38 of 60
running inference on chip 39 of 60
running inference on chip 40 of 60
running inference on chip 41 of 60
running inference on chip 42 of 60
running inference on chip 43 of 60
running inference on chip 44 of 60
running inference on chip 45 of 60
running inference on chip 46 of 60
running inference on chip 47 of 60
running inference on chip 48 of 60
running inference on chip 49 of 60
running inference on chip 50 of 60
running inference on chip 51 of 60
running inference on chip 52 of 60
running inference on chip 53 of 60
running inference on chip 54 of 60
running inference on chip 55 of 60
running inference on chip 56 of 60
running inference on chip 57 of 60
running inference on chip 58 of 60
running inference on chip 59 of 60
running inference on image dataset-medium/images/39e77bedd0_729FB913CDOPENPIPELINE-ortho.tif.
loading input image (6497, 6306, 3)
running inference on chip 0 of 36
running inference on chip 1 of 36
running inference on chip 2 of 36
running inference on chip 3 of 36
running inference on chip 4 of 36
running inference on chip 5 of 36
running inference on chip 6 of 36
running inference on chip 7 of 36
running inference on chip 8 of 36
running inference on chip 9 of 36
running inference on chip 10 of 36
running inference on chip 11 of 36
running inference on chip 12 of 36
running inference on chip 13 of 36
running inference on chip 14 of 36
running inference on chip 15 of 36
running inference on chip 16 of 36
running inference on chip 17 of 36
running inference on chip 18 of 36
running inference on chip 19 of 36
running inference on chip 20 of 36
running inference on chip 21 of 36
running inference on chip 22 of 36
running inference on chip 23 of 36
running inference on chip 24 of 36
running inference on chip 25 of 36
running inference on chip 26 of 36
running inference on chip 27 of 36
running inference on chip 28 of 36
running inference on chip 29 of 36
running inference on chip 30 of 36
running inference on chip 31 of 36
running inference on chip 32 of 36
running inference on chip 33 of 36
running inference on chip 34 of 36
running inference on chip 35 of 36
running inference on image dataset-medium/images/e87da4ebdb_29FEA32BC7INSPIRE-ortho.tif.
loading input image (13185, 12230, 3)
running inference on chip 0 of 121
running inference on chip 1 of 121
running inference on chip 2 of 121
running inference on chip 3 of 121
running inference on chip 4 of 121
running inference on chip 5 of 121
running inference on chip 6 of 121
running inference on chip 7 of 121
running inference on chip 8 of 121
running inference on chip 9 of 121
running inference on chip 10 of 121
running inference on chip 11 of 121
running inference on chip 12 of 121
running inference on chip 13 of 121
running inference on chip 14 of 121
running inference on chip 15 of 121
running inference on chip 16 of 121
running inference on chip 17 of 121
running inference on chip 18 of 121
running inference on chip 19 of 121
running inference on chip 20 of 121
running inference on chip 21 of 121
running inference on chip 22 of 121
running inference on chip 23 of 121
running inference on chip 24 of 121
running inference on chip 25 of 121
running inference on chip 26 of 121
running inference on chip 27 of 121
running inference on chip 28 of 121
running inference on chip 29 of 121
running inference on chip 30 of 121
running inference on chip 31 of 121
running inference on chip 32 of 121
running inference on chip 33 of 121
running inference on chip 34 of 121
running inference on chip 35 of 121
running inference on chip 36 of 121
running inference on chip 37 of 121
running inference on chip 38 of 121
running inference on chip 39 of 121
running inference on chip 40 of 121
running inference on chip 41 of 121
running inference on chip 42 of 121
running inference on chip 43 of 121
running inference on chip 44 of 121
running inference on chip 45 of 121
running inference on chip 46 of 121
running inference on chip 47 of 121
running inference on chip 48 of 121
running inference on chip 49 of 121
running inference on chip 50 of 121
running inference on chip 51 of 121
running inference on chip 52 of 121
running inference on chip 53 of 121
running inference on chip 54 of 121
running inference on chip 55 of 121
running inference on chip 56 of 121
running inference on chip 57 of 121
running inference on chip 58 of 121
running inference on chip 59 of 121
running inference on chip 60 of 121
running inference on chip 61 of 121
running inference on chip 62 of 121
running inference on chip 63 of 121
running inference on chip 64 of 121
running inference on chip 65 of 121
running inference on chip 66 of 121
running inference on chip 67 of 121
running inference on chip 68 of 121
running inference on chip 69 of 121
running inference on chip 70 of 121
running inference on chip 71 of 121
running inference on chip 72 of 121
running inference on chip 73 of 121
running inference on chip 74 of 121
running inference on chip 75 of 121
running inference on chip 76 of 121
running inference on chip 77 of 121
running inference on chip 78 of 121
running inference on chip 79 of 121
running inference on chip 80 of 121
running inference on chip 81 of 121
running inference on chip 82 of 121
running inference on chip 83 of 121
running inference on chip 84 of 121
running inference on chip 85 of 121
running inference on chip 86 of 121
running inference on chip 87 of 121
running inference on chip 88 of 121
running inference on chip 89 of 121
running inference on chip 90 of 121
running inference on chip 91 of 121
running inference on chip 92 of 121
running inference on chip 93 of 121
running inference on chip 94 of 121
running inference on chip 95 of 121
running inference on chip 96 of 121
running inference on chip 97 of 121
running inference on chip 98 of 121
running inference on chip 99 of 121
running inference on chip 100 of 121
running inference on chip 101 of 121
running inference on chip 102 of 121
running inference on chip 103 of 121
running inference on chip 104 of 121
running inference on chip 105 of 121
running inference on chip 106 of 121
running inference on chip 107 of 121
running inference on chip 108 of 121
running inference on chip 109 of 121
running inference on chip 110 of 121
running inference on chip 111 of 121
running inference on chip 112 of 121
running inference on chip 113 of 121
running inference on chip 114 of 121
running inference on chip 115 of 121
running inference on chip 116 of 121
running inference on chip 117 of 121
running inference on chip 118 of 121
running inference on chip 119 of 121
running inference on chip 120 of 121
running inference on image dataset-medium/images/420d6b69b8_84B52814D2OPENPIPELINE-ortho.tif.
loading input image (9364, 9555, 3)
running inference on chip 0 of 64
running inference on chip 1 of 64
running inference on chip 2 of 64
running inference on chip 3 of 64
running inference on chip 4 of 64
running inference on chip 5 of 64
running inference on chip 6 of 64
running inference on chip 7 of 64
running inference on chip 8 of 64
running inference on chip 9 of 64
running inference on chip 10 of 64
running inference on chip 11 of 64
running inference on chip 12 of 64
running inference on chip 13 of 64
running inference on chip 14 of 64
running inference on chip 15 of 64
running inference on chip 16 of 64
running inference on chip 17 of 64
running inference on chip 18 of 64
running inference on chip 19 of 64
running inference on chip 20 of 64
running inference on chip 21 of 64
running inference on chip 22 of 64
running inference on chip 23 of 64
running inference on chip 24 of 64
running inference on chip 25 of 64
running inference on chip 26 of 64
running inference on chip 27 of 64
running inference on chip 28 of 64
running inference on chip 29 of 64
running inference on chip 30 of 64
running inference on chip 31 of 64
running inference on chip 32 of 64
running inference on chip 33 of 64
running inference on chip 34 of 64
running inference on chip 35 of 64
running inference on chip 36 of 64
running inference on chip 37 of 64
running inference on chip 38 of 64
running inference on chip 39 of 64
running inference on chip 40 of 64
running inference on chip 41 of 64
running inference on chip 42 of 64
running inference on chip 43 of 64
running inference on chip 44 of 64
running inference on chip 45 of 64
running inference on chip 46 of 64
running inference on chip 47 of 64
running inference on chip 48 of 64
running inference on chip 49 of 64
running inference on chip 50 of 64
running inference on chip 51 of 64
running inference on chip 52 of 64
running inference on chip 53 of 64
running inference on chip 54 of 64
running inference on chip 55 of 64
running inference on chip 56 of 64
running inference on chip 57 of 64
running inference on chip 58 of 64
running inference on chip 59 of 64
running inference on chip 60 of 64
running inference on chip 61 of 64
running inference on chip 62 of 64
running inference on chip 63 of 64
running inference on image dataset-medium/images/d06b2c67d2_2A62B67B52OPENPIPELINE-ortho.tif.
loading input image (6879, 5351, 3)
running inference on chip 0 of 30
running inference on chip 1 of 30
running inference on chip 2 of 30
running inference on chip 3 of 30
running inference on chip 4 of 30
running inference on chip 5 of 30
running inference on chip 6 of 30
running inference on chip 7 of 30
running inference on chip 8 of 30
running inference on chip 9 of 30
running inference on chip 10 of 30
running inference on chip 11 of 30
running inference on chip 12 of 30
running inference on chip 13 of 30
running inference on chip 14 of 30
running inference on chip 15 of 30
running inference on chip 16 of 30
running inference on chip 17 of 30
running inference on chip 18 of 30
running inference on chip 19 of 30
running inference on chip 20 of 30
running inference on chip 21 of 30
running inference on chip 22 of 30
running inference on chip 23 of 30
running inference on chip 24 of 30
running inference on chip 25 of 30
running inference on chip 26 of 30
running inference on chip 27 of 30
running inference on chip 28 of 30
running inference on chip 29 of 30
running inference on image dataset-medium/images/107f24d6e9_F1BE1D4184INSPIRE-ortho.tif.
loading input image (12326, 11084, 3)
running inference on chip 0 of 110
running inference on chip 1 of 110
running inference on chip 2 of 110
running inference on chip 3 of 110
running inference on chip 4 of 110
running inference on chip 5 of 110
running inference on chip 6 of 110
running inference on chip 7 of 110
running inference on chip 8 of 110
running inference on chip 9 of 110
running inference on chip 10 of 110
running inference on chip 11 of 110
running inference on chip 12 of 110
running inference on chip 13 of 110
running inference on chip 14 of 110
running inference on chip 15 of 110
running inference on chip 16 of 110
running inference on chip 17 of 110
running inference on chip 18 of 110
running inference on chip 19 of 110
running inference on chip 20 of 110
running inference on chip 21 of 110
running inference on chip 22 of 110
running inference on chip 23 of 110
running inference on chip 24 of 110
running inference on chip 25 of 110
running inference on chip 26 of 110
running inference on chip 27 of 110
running inference on chip 28 of 110
running inference on chip 29 of 110
running inference on chip 30 of 110
running inference on chip 31 of 110
running inference on chip 32 of 110
running inference on chip 33 of 110
running inference on chip 34 of 110
running inference on chip 35 of 110
running inference on chip 36 of 110
running inference on chip 37 of 110
running inference on chip 38 of 110
running inference on chip 39 of 110
running inference on chip 40 of 110
running inference on chip 41 of 110
running inference on chip 42 of 110
running inference on chip 43 of 110
running inference on chip 44 of 110
running inference on chip 45 of 110
running inference on chip 46 of 110
running inference on chip 47 of 110
running inference on chip 48 of 110
running inference on chip 49 of 110
running inference on chip 50 of 110
running inference on chip 51 of 110
running inference on chip 52 of 110
running inference on chip 53 of 110
running inference on chip 54 of 110
running inference on chip 55 of 110
running inference on chip 56 of 110
running inference on chip 57 of 110
running inference on chip 58 of 110
running inference on chip 59 of 110
running inference on chip 60 of 110
running inference on chip 61 of 110
running inference on chip 62 of 110
running inference on chip 63 of 110
running inference on chip 64 of 110
running inference on chip 65 of 110
running inference on chip 66 of 110
running inference on chip 67 of 110
running inference on chip 68 of 110
running inference on chip 69 of 110
running inference on chip 70 of 110
running inference on chip 71 of 110
running inference on chip 72 of 110
running inference on chip 73 of 110
running inference on chip 74 of 110
running inference on chip 75 of 110
running inference on chip 76 of 110
running inference on chip 77 of 110
running inference on chip 78 of 110
running inference on chip 79 of 110
running inference on chip 80 of 110
running inference on chip 81 of 110
running inference on chip 82 of 110
running inference on chip 83 of 110
running inference on chip 84 of 110
running inference on chip 85 of 110
running inference on chip 86 of 110
running inference on chip 87 of 110
running inference on chip 88 of 110
running inference on chip 89 of 110
running inference on chip 90 of 110
running inference on chip 91 of 110
running inference on chip 92 of 110
running inference on chip 93 of 110
running inference on chip 94 of 110
running inference on chip 95 of 110
running inference on chip 96 of 110
running inference on chip 97 of 110
running inference on chip 98 of 110
running inference on chip 99 of 110
running inference on chip 100 of 110
running inference on chip 101 of 110
running inference on chip 102 of 110
running inference on chip 103 of 110
running inference on chip 104 of 110
running inference on chip 105 of 110
running inference on chip 106 of 110
running inference on chip 107 of 110
running inference on chip 108 of 110
running inference on chip 109 of 110
running inference on image dataset-medium/images/1726eb08ef_60693DB04DINSPIRE-ortho.tif.
loading input image (3440, 2867, 3)
running inference on chip 0 of 9
running inference on chip 1 of 9
running inference on chip 2 of 9
running inference on chip 3 of 9
running inference on chip 4 of 9
running inference on chip 5 of 9
running inference on chip 6 of 9
running inference on chip 7 of 9
running inference on chip 8 of 9
running inference on image dataset-medium/images/dabec5e872_E8AD935CEDINSPIRE-ortho.tif.
loading input image (9746, 9364, 3)
running inference on chip 0 of 72
running inference on chip 1 of 72
running inference on chip 2 of 72
running inference on chip 3 of 72
running inference on chip 4 of 72
running inference on chip 5 of 72
running inference on chip 6 of 72
running inference on chip 7 of 72
running inference on chip 8 of 72
running inference on chip 9 of 72
running inference on chip 10 of 72
running inference on chip 11 of 72
running inference on chip 12 of 72
running inference on chip 13 of 72
running inference on chip 14 of 72
running inference on chip 15 of 72
running inference on chip 16 of 72
running inference on chip 17 of 72
running inference on chip 18 of 72
running inference on chip 19 of 72
running inference on chip 20 of 72
running inference on chip 21 of 72
running inference on chip 22 of 72
running inference on chip 23 of 72
running inference on chip 24 of 72
running inference on chip 25 of 72
running inference on chip 26 of 72
running inference on chip 27 of 72
running inference on chip 28 of 72
running inference on chip 29 of 72
running inference on chip 30 of 72
running inference on chip 31 of 72
running inference on chip 32 of 72
running inference on chip 33 of 72
running inference on chip 34 of 72
running inference on chip 35 of 72
running inference on chip 36 of 72
running inference on chip 37 of 72
running inference on chip 38 of 72
running inference on chip 39 of 72
running inference on chip 40 of 72
running inference on chip 41 of 72
running inference on chip 42 of 72
running inference on chip 43 of 72
running inference on chip 44 of 72
running inference on chip 45 of 72
running inference on chip 46 of 72
running inference on chip 47 of 72
running inference on chip 48 of 72
running inference on chip 49 of 72
running inference on chip 50 of 72
running inference on chip 51 of 72
running inference on chip 52 of 72
running inference on chip 53 of 72
running inference on chip 54 of 72
running inference on chip 55 of 72
running inference on chip 56 of 72
running inference on chip 57 of 72
running inference on chip 58 of 72
running inference on chip 59 of 72
running inference on chip 60 of 72
running inference on chip 61 of 72
running inference on chip 62 of 72
running inference on chip 63 of 72
running inference on chip 64 of 72
running inference on chip 65 of 72
running inference on chip 66 of 72
running inference on chip 67 of 72
running inference on chip 68 of 72
running inference on chip 69 of 72
running inference on chip 70 of 72
running inference on chip 71 of 72
precision=0.926083988860852 recall=0.43464161034909143 f1=0.5790180521000616 miou=0.13868623249100406 iou per class=[0.011886 0.013628 0.100129 0.071405 0.4588   0.176269]
Length of iou array is: 6 for 12fa5e614f_53197F206FOPENPIPELINE
precision=0.9350152467339494 recall=0.5757965527363759 f1=0.7098744670906811 miou=0.2609914621840612 iou per class=[0.       0.112891 0.489349 0.       0.560998 0.402711]
Length of iou array is: 6 for c2e8370ca3_3340CAC7AEOPENPIPELINE
precision=0.7958278738524396 recall=0.7650819822208302 f1=0.7782830041102974 miou=0.4397654102299184 iou per class=[0.586133 0.062739 0.759331 0.       0.660985 0.569405]
Length of iou array is: 6 for 6f93b9026b_F1BFB8B17DOPENPIPELINE
precision=0.8099344062295936 recall=0.6294558801866812 f1=0.6722513846828545 miou=0.32177072825909475 iou per class=[0.698451 0.291553 0.268096 0.047205 0.561526 0.063794]
Length of iou array is: 6 for 8710b98ea0_06E6522D6DINSPIRE
precision=0.74685351282094 recall=0.5779170105090748 f1=0.6228447327532964 miou=0.23175670722706462 iou per class=[0.219912 0.036058 0.508321 0.       0.541099 0.08515 ]
Length of iou array is: 6 for 25f1c24f30_EB81FE6E2BOPENPIPELINE
precision=0.6151330205842467 recall=0.6371206578755928 f1=0.607290864717393 miou=0.12906602777707119 iou per class=[0.005037 0.015662 0.108278 0.       0.645418 0.      ]
Length of iou array is: 6 for 39e77bedd0_729FB913CDOPENPIPELINE
precision=0.927167152745086 recall=0.8821893572431956 f1=0.8973429002445896 miou=0.2962710622627594 iou per class=[0.       0.05747  0.561995 0.277743 0.880418 0.      ]
Length of iou array is: 6 for e87da4ebdb_29FEA32BC7INSPIRE
precision=0.88812539113517 recall=0.8606329274848451 f1=0.8673447502904136 miou=0.41467691604624424 iou per class=[0.51257  0.106797 0.848028 0.016356 0.753304 0.251007]
Length of iou array is: 6 for 420d6b69b8_84B52814D2OPENPIPELINE
precision=0.8867567371214875 recall=0.8737595901652879 f1=0.8479463395566713 miou=0.234444426649689 iou per class=[0.000000e+00 4.990712e-02 2.055069e-01 5.783481e-04 8.712641e-01 2.794101e-01]
Length of iou array is: 6 for d06b2c67d2_2A62B67B52OPENPIPELINE
precision=0.7975269136160227 recall=0.7752550687315124 f1=0.7816213302622909 miou=0.37718288760206 iou per class=[0.637961 0.038052 0.483554 0.199073 0.734352 0.170106]
Length of iou array is: 6 for 107f24d6e9_F1BE1D4184INSPIRE
precision=0.6850534407300328 recall=0.5981129444159292 f1=0.6273941287772274 miou=0.30216736275767153 iou per class=[0.483654 0.126367 0.56442  0.       0.475429 0.163135]
Length of iou array is: 6 for 1726eb08ef_60693DB04DINSPIRE
precision=0.9016176093007324 recall=0.8656495353827639 f1=0.8725925504840398 miou=0.3767779417174994 iou per class=[0.467538 0.090342 0.364502 0.057318 0.861027 0.41994 ]
Length of iou array is: 6 for dabec5e872_E8AD935CEDINSPIRE
{'mean_iou': 0.2936297637670115, 'miou_class': array([0.301928, 0.083455, 0.438459, 0.055807, 0.667052, 0.215077]), 'f1_mean': 0.7386503754224848, 'f1_std': 0.11135996425819042, 'pr_mean': 0.8262579411442127, 'pr_std': 0.09915402604107505, 're_mean': 0.7063010931084318, 're_std': 0.1432095468519169}
/home/TUE/20176671/miniconda3/envs/fastai/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/mnt/server-home/TUE/20176671/dd-ml-segmentation-benchmark/libs/scoring.py:53: RuntimeWarning: invalid value encountered in true_divide
  cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
